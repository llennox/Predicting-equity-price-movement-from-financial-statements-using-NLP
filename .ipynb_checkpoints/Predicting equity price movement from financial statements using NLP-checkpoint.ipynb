{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['upper_quantile_price_change', 'lower_quantile_price_change',\n",
       "       'percise_date', 'id', 'symbol', 'marketcap', 'industry',\n",
       "       'yfinance_symbol_id', 'item_types', 'clean_content', 'split_sents',\n",
       "       'split_sent_length', 'industry_code', 'pct_change_close',\n",
       "       'pct_change_open', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('with_hours.csv', compression = 'gzip')\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we'll load the csv that contains 13,000 filings since 2016, here's a quick explination of the columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upper_quantile_price_change: highest 75% of overnight price movement for the equity  \n",
    "lower_quantile_price_change: lowest 25% of overnight price movement for the equity  \n",
    "percise_date: the datetime the filing was published  \n",
    "id: filing text id in my db @  \n",
    "symbol: the trading symbol for the equity @  \n",
    "marketcap: the market cap *  \n",
    "industry: the industry * @  \n",
    "yfinance_symbol_id: id of the symbol in my db   \n",
    "item_types: the item types attached to each filing @  \n",
    "clean_content: the 'other events' section text only   \n",
    "split_sents: clean_content split into sentences with the shape [( sentence, filing_text_id ), ...]  \n",
    "split_sent_length: the number of tuples in each split_sents list   \n",
    "industry_code: the categorical numerical representation of the industry  \n",
    "pct_change_close: the percent change since last close the day of or after the filing publish @  \n",
    "pct_change_open: the percent change difference between last close and the next morning @  \n",
    "category: categorical representation of pct_change_open  \n",
    "            ```conditions = [\n",
    "                (df['pct_change_open'] <= df['lower_quantile_price_change']),\n",
    "                (df['pct_change_open'] >=  df['upper_quantile_price_change'])\n",
    "            ]\n",
    "            values = [1, 2]\n",
    "            df['category'] = np.select(conditions, values, default=0)```\n",
    "\n",
    "* = provided by yahoo finance  \n",
    "@ = isn't used in the model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-be3cc335a05d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentencizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentencizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentencizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "sentencizer = self.nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(self.sentencizer)\n",
    "\n",
    "def split_text(row):\n",
    "    return_content = []\n",
    "    doc = nlp(row['clean_content'])\n",
    "    for s in doc.sents:\n",
    "        if len(s.text.split()) > 5:\n",
    "            return_content.append(( correct_spaces(s.text), row['id'] ))\n",
    "    return return_conten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after processing we want to use https://github.com/UKPLab/sentence-transformers to create bert embeddings for each sentence in our split_sents tuples. Bert embeddings are vector representation of sentences. Interestingly enough cosine distance between the embeddings also indicate semantic similarity. We can use this property to make clusters of sentences with similar meaning without supervision. The embedding process can take a while, I recommend selecting a subset of the dataframe to start on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional\n",
    "df = df[df['industry'] == 'Biotechnology']\n",
    "df = df[df['category'] != 0] # this is cheating, we're technically looking ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[14:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "full_sentence_vectors = []\n",
    "for i, row in df.iterrows():\n",
    "    full_sentence_vectors.extend(row['split_sents'])\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens') # automatically download this model\n",
    "\n",
    "embedded = model.encode([x[0] for x in full_sentence_vectors])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1424"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our embeddings we're going to want to reduce the dimensionality with umap learn and cluster with hdbscan, we'll then create a column for each cluster that is a boolean representation of if the filing text contains a sentence belonging to that cluster. These params are the ones that have worked best for me but feel free to try new ones out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_neighbors=15,\n",
    "                    n_components=100,\n",
    "                    min_dist=0.1,\n",
    "                    low_memory=True,\n",
    "                    angular_rp_forest=False,\n",
    "                    metric='cosine')\n",
    "umap_model.fit(embedded)\n",
    "\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=50,\n",
    "                        metric='euclidean',\n",
    "                        prediction_data=True,\n",
    "                        cluster_selection_method='eom')\n",
    "cluster.fit(umap_model.embedding_)\n",
    "\n",
    "filing_text_ids_cluster_labels_zip = zip([x[1] for x in full_sentence_vectors], cluster.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before we go further let's take a quick detour and visualize with umap plot, we'll need to reduce our embeddings to 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_neighbors=15,\n",
    "                    n_components=2,\n",
    "                    min_dist=0.1,\n",
    "                    low_memory=True,\n",
    "                    angular_rp_forest=False,\n",
    "                    metric='cosine')\n",
    "umap_model.fit(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `filing_text_ids_cluster_labels_zip` contains tuples with the values ( dataframe id, cluster id ) for each sentence throughout the filing texts. Now we'll iterate through it and add the cluster columns for our decision tree to make predictions on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for filing_text_id, cluster_label in filing_text_ids_cluster_labels_zip:\n",
    "    if f'cluster_id_{cluster_label}' not in df:\n",
    "        df[f'cluster_id_{cluster_label}'] = 0\n",
    "    idx = df.index[df['id'] == filing_text_id][0]\n",
    "    df.loc[idx, f'cluster_id_{cluster_label}'] = 1\n",
    "    \n",
    "y = df['category'].values # create our target\n",
    "\n",
    "x = df.drop(columns=['id', 'pct_change_open', 'clean_content', 'category', 'symbol', 'industry', 'split_sents', 'item_types', 'cluster_id_-1', 'pct_change_close'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to train our decision tree. First we'll split into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.02, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=1250,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=2,\n",
    "                                max_features= 'auto',\n",
    "                                max_depth=None,\n",
    "                                bootstrap=True)\n",
    "model.fit(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
